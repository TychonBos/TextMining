{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "    \n",
    "    return match_list, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = e_type\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            text = clean(text)\n",
    "            text_ = text        \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                if a:\n",
    "                    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "                    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = mark_sentence(text, match_list)\n",
    "\n",
    "            for i in d.keys():\n",
    "                f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data = pd.DataFrame(columns=['text', 'annotation'])\n",
    "test_data = pd.DataFrame(columns=['text', 'annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_annotation = pd.read_json('train_val.json')\n",
    "test_annotations = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gt(df):\n",
    "\n",
    "    ground_truth = []\n",
    "\n",
    "    for id in df['id'].unique():\n",
    "        latest_update = df[df['id'] == id]['updated_at'].max()\n",
    "        gt = df[(df['id'] == id)&(df['updated_at'] == latest_update)]\n",
    "        ground_truth.append(gt)\n",
    "\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_gt = extract_gt(train_val_annotation)\n",
    "test_gt = extract_gt(test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framer(gt):\n",
    "    data = pd.DataFrame(columns=['text', 'annotation'])\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    for line in gt:\n",
    "        index = 0\n",
    "\n",
    "        text = list(line['text'])[0]  # Assuming 'text' is a key in your dictionary\n",
    "        \n",
    "        for s in tokenizer.tokenize(text):\n",
    "            sorted_line = sorted(list(line['label'])[0], key=lambda x: x['start'])\n",
    "            sentence_entities = []\n",
    "            \n",
    "            for l in sorted_line:\n",
    "                start = l['start']\n",
    "                end = l['end']\n",
    "                \n",
    "                if start >= index and end <= len(s) + index:\n",
    "                    entity_text = l['text']\n",
    "                    entity_label = l['labels'][0]\n",
    "                    sentence_entities.append((entity_text, entity_label))\n",
    "            \n",
    "            if sentence_entities:\n",
    "                data = data.append({'text': s, 'annotation': sentence_entities}, ignore_index=True)\n",
    "            \n",
    "            index += len(s)\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data = framer(train_val_gt)\n",
    "test_data = framer(test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_val_data[:int(len(train_val_data)*0.80)]\n",
    "val_data = train_val_data[int(len(train_val_data)*0.80):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = 'flair_data/train.txt'\n",
    "val_filepath = 'flair_data/val.txt'\n",
    "test_filepath = 'flair_data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data(train_data, train_filepath)\n",
    "create_data(val_data, val_filepath)\n",
    "create_data(test_data, test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "# directory where the data resides\n",
    "data_folder = 'flair_data/'\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt',\n",
    "                              test_file = 'test.txt',\n",
    "                              dev_file = 'val.txt')\n",
    "\n",
    "embedding_types = [\n",
    "    WordEmbeddings('glove'),  # You can add more embeddings if needed\n",
    "]\n",
    "\n",
    "# Create Stacked Embeddings\n",
    "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# Create a SequenceTagger model\n",
    "tag_type = 'ner'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "    \n",
    "model = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)\n",
    "\n",
    "# Create a ModelTrainer and fine-tune the model\n",
    "trainer = ModelTrainer(model, corpus)\n",
    "\n",
    "trainer.train(\n",
    "    'flair_models',\n",
    "    learning_rate=0.1,\n",
    "    mini_batch_size=64,\n",
    "    max_epochs=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Load your fine-tuned model\n",
    "custom_ner_model = SequenceTagger.load(r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\flair_models\\best-model.pt')\n",
    "\n",
    "# Create a Sentence for NER\n",
    "sentence = Sentence(\"Brandon Aguilera Zamora (born 28 June 2003) is a Costa Rican professional footballer who plays as a midfielder for Premier League club Nottingham Forest and the Costa Rica national team. Career. Club. In July 2022 Premier League side Nottingham Forest announced they had signed Aguilera on a four-year deal from Alajuelense, and would immediately be loaned to fellow Costa Rican side Guanacasteca for six months. In January 2023, Aguilera joined Primeira Liga club Estoril on loan until the end of the season. He played with the teams under 23 squad. International. A youth international for Costa Rica since 2018, Aguilera made his senior team debut against the United States on 30 March 2022. In November 2022 he was named to the 26-man Costa Rica squad for the 2022 FIFA World Cup. Honours. Alajuelense\")\n",
    "\n",
    "# Run NER on the sentence\n",
    "custom_ner_model.predict(sentence)\n",
    "\n",
    "# Access NER results\n",
    "print(sentence.text)\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:00:15,402 SequenceTagger predicts: Dictionary with 10 tags: O, PLAYER, BIRTHDATE, COUNTRY, NATIONALITY, POSITION, CLUB, REFERENCE, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Load your fine-tuned model\n",
    "custom_ner_model = SequenceTagger.load(r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\flair_models\\best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def output_to_json(s, output_file):\n",
    "    output = dict()\n",
    "    output['text'] = s.text\n",
    "    result = []\n",
    "    idx = 0\n",
    "    for i in range(len(s.labels)):\n",
    "        token, label = s.labels[i].labeled_identifier.split('/')\n",
    "        token = token.replace('\"', '')\n",
    "        token = token.split(' ')[1]\n",
    "        start = output['text'].find(token, idx)\n",
    "        end = start + len(token)\n",
    "        idx = end\n",
    "        v = {'end':end, 'text':token, 'start':start, 'labels':[label]}\n",
    "        result.append(v)\n",
    "    output['label'] = result\n",
    "    with open(output_file, 'w') as fp:\n",
    "        json.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_json(r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\c1_data\\test_c1.json')\n",
    "bench = extract_gt(bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in enumerate(bench):\n",
    "    s = Sentence(bench[idx].text.iloc[0])\n",
    "    custom_ner_model.predict(s)\n",
    "    output_to_json(s, r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\c1_output\\file'+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def augmented_text(sentence):\n",
    "    outp = sentence.text\n",
    "    used_text = []\n",
    "    for l in sentence.labels:\n",
    "        text_token = l.labeled_identifier\n",
    "        text_token = text_token.split()\n",
    "        text, token = text_token[1].split('/')\n",
    "        text = text.replace('\"', '')\n",
    "        if text not in used_text:\n",
    "            outp = outp.replace(text, f'[{token}]{text}[{token}]')\n",
    "            outp = outp.replace(f'[{token}] [{token}]', ' ')\n",
    "            used_text.append(text)\n",
    "        else: \n",
    "            continue\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
