{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the Premier League players page\n",
    "base_url = \"https://www.worldfootball.net/players_list/eng-premier-league-2023-2024/nach-name/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all players\n",
    "all_players = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the URLs and send HTTP GET requests\n",
    "for i in range(1, 13):\n",
    "    url = base_url + str(i) + \"/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find('table', class_='standard_tabelle')\n",
    "        \n",
    "        if table:\n",
    "            # Find all the table rows (tr) within the table with class 'standard_tabelle'\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            # Loop through the rows and extract player names\n",
    "            for row in rows:\n",
    "                # Check if the row contains a link (a) with player information\n",
    "                player_link = row.find('a', href=True)\n",
    "                \n",
    "                if player_link:\n",
    "                    # Extract the player name and add it to the list\n",
    "                    player_name = player_link.get_text()\n",
    "                    all_players.append(player_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all strings to lower case\n",
    "all_players = list(map(lambda x: x.lower(), all_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories containing wikipedia articles\n",
    "dirA = r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\enwiki20230820-stripped-json\\enwiki20230820-stripped-json\\AA'\n",
    "dirB = r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\enwiki20230820-stripped-json\\enwiki20230820-stripped-json\\AB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate list that will contain the wikipedia articles about the players\n",
    "player_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all files and selecting the lines whereby the title equals the name of the football players\n",
    "for dir in [dirA, dirB]:\n",
    "    for file in os.listdir(dir):\n",
    "        with open(dir+'\\\\'+file, 'r') as fp:\n",
    "            while True:\n",
    "                line = fp.readline()\n",
    "                if line == '':\n",
    "                    fp.close()\n",
    "                    break\n",
    "                line = json.loads(line)\n",
    "                title = line['title'].lower()\n",
    "                if title in all_players:\n",
    "                    player_articles.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 20 random indices\n",
    "random_indices = random.sample(range(1, len(player_articles)), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 20 random players\n",
    "twenty_random_players = [player_articles[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating files that contain the wikipedia content\n",
    "for player in twenty_random_players:\n",
    "    title = player['title']\n",
    "    fp = open(r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\football_articles'+'\\\\'+title+'.txt', 'w')\n",
    "    text = player['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    fp.write(text)\n",
    "    fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
